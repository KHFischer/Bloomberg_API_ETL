import numpy as np
import pandas as pd
import string
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer


def transform_article(article_df):

    # Replace empty strings with NaN
    article_df.replace('', np.nan, inplace=True)

    # Unpack lists into strings
    article_df['secondaryBrands'] = article_df['secondaryBrands'].apply(lambda y: y if len(y) != 1 else y[0])
    article_df['byline'] = article_df['byline'].apply(lambda y: y.split('and'))
    article_df['byline'] = article_df['byline'].apply(lambda y: y if len(y) != 1 else y[0])

    # Extract dates from URL
    article_df['date'] = article_df['shortUrl'].apply(lambda y: y.split('/')[5])

    # Extract year, month, day from date
    article_df['year'] = article_df['date'].apply(lambda y: y.split('-')[0])
    article_df['month'] = article_df['date'].apply(lambda y: y.split('-')[1])
    article_df['day'] = article_df['date'].apply(lambda y: y.split('-')[2])

    # Turn date into datetime object
    article_df['date'] = pd.to_datetime(article_df['date'])

    # Combine text columns into 1 combined text column
    article_df['combinedText'] = article_df['title'].astype(str) + ' ' + article_df['summary'].astype(str) + ' ' + \
                                 article_df['autoGeneratedSummary'].astype(str) + ' ' + article_df[
                                     'secondaryBrands'].astype(str) + ' ' + article_df['byline'].astype(str) + ' ' + \
                                 article_df['primaryCategory'].astype(str) + ' ' + article_df['primarySite'].astype(
        str) + ' ' + article_df['card'].astype(str)

    # Drop text columns as they are represented in combined column
    article_df.drop(
        ['title', 'summary', 'shortUrl', 'autoGeneratedSummary', 'secondaryBrands', 'byline', 'primaryCategory',
         'primarySite'], axis=1, inplace=True)

    return article_df


def transform_video(video_df):
    # Similar process as transform_article

    video_df['date'] = video_df['shortUrl'].apply(lambda y: y.split('/')[5])
    video_df['year'] = video_df['date'].apply(lambda y: y.split('-')[0])
    video_df['month'] = video_df['date'].apply(lambda y: y.split('-')[1])
    video_df['day'] = video_df['date'].apply(lambda y: y.split('-')[2])
    video_df['date'] = pd.to_datetime(video_df['date'])

    video_df['combinedText'] = video_df['title'].astype(str) + ' ' + video_df['summary'].astype(str) + ' ' + video_df[
        'primaryCategory'].astype(str) + ' ' + video_df['channel'].astype(str) + ' ' + video_df['card'].astype(str)
    video_df.drop(['title', 'summary', 'primaryCategory', 'channel', 'shortUrl'], axis=1, inplace=True)

    return video_df


def consolidate(article_df, video_df):
    df = pd.concat([article_df, video_df])
    return df


def cleaning(text):
    clean_text = text.translate(str.maketrans('', '', string.punctuation)).lower()
    clean_text = [word for word in clean_text.split() if word not in stopwords.words('english')]

    sentence = []
    for word in clean_text:
        lemmatizer = WordNetLemmatizer()
        sentence.append(lemmatizer.lemmatize(word, 'v'))

    return ' '.join(sentence)


def main():
    print('[Transform] Start')
    print('[Transform] Transforming articles')
    article_df = transform_article()

    print('[Transform] Transforming videos')
    video_df = transform_video()

    print('[Transform] Consolidating data')
    df = consolidate()

    print('[Transform] Cleaning data')
    df['cleanedText'] = df['combinedText'].apply(cleaning)
    df.drop('combinedText', axis=1, inplace=True)

    print('[Transform] End.')
